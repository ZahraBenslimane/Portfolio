<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://zahrabenslimane.github.io/Portfolio/feed.xml" rel="self" type="application/atom+xml" /><link href="https://zahrabenslimane.github.io/Portfolio/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-06-17T20:37:15+00:00</updated><id>https://zahrabenslimane.github.io/Portfolio/feed.xml</id><title type="html">Zahra Benslimane</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Brief Summary | Distilling the Knowledge in a Neural Network</title><link href="https://zahrabenslimane.github.io/Portfolio/blog/2023/KD/" rel="alternate" type="text/html" title="Brief Summary | Distilling the Knowledge in a Neural Network" /><published>2023-05-07T10:40:16+00:00</published><updated>2023-05-07T10:40:16+00:00</updated><id>https://zahrabenslimane.github.io/Portfolio/blog/2023/KD</id><content type="html" xml:base="https://zahrabenslimane.github.io/Portfolio/blog/2023/KD/"><![CDATA[<p>Paper : <a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a></p>

<p>The authors proposed an architecture-agnostic technique called knowledge distillation, which involves training a
small model to mimic the behavior of a cumbersome model, referring to a single large model or an ensemble of
models. To train the small model to generalize in the same way as the cumbersome model, it is as important to
learn the target outputs of the model, as it is to learn the ratios of the small probabilities of the incorrect targets.
However, these small probabilities tend to be suppressed by the softmax function that greatly highlights the value of
the maximum output logit, minimizing their influence on the cross-entropy cost function. The authors figured that
by raising the temperature parameter in the final softmax function, it produces a softer probability distribution over
classes, called soft targets, resulting in a higher entropy, thus, more information to learn from.</p>

<p>The main idea is to first train a large network with a softmax temperature equal to 1, then feed a transfer set of unlabeled or labeled data to this model and compute their soft targets with a temperature T, higher than one. Knowledge is
then transferred to the small model, by training it to align its soft targets, computed with the same high-temperature
T used for generating the transfer set, with the soft targets produced by the cumbersome model. A weighted average
of two objective functions is used for training the distilled model, the cross entropy of the soft targets of the two
models, and the cross entropy of the distilled model with the correct target class.</p>

<p>Additionally, the paper presents a creative way of training big ensembles as our cumbersome model. A generalist
model is trained on all training data classes and multiple specialist models are trained in parallel to focus on clustered
subsets that are more confusable to the generalist model.</p>

<p>The paper highlights the fact that different requirements are set for training and inferring neural network models and
encourages first focusing on achieving the best performance of the model regardless of the computational cost and
then transferring the knowledge to a more resource-contrained network. Knowledge distillation is general enough
to be applied to different fields and has shown great results in compressing model architecture and transferring
the knowledge of an ensemble of models into a single network, without compromising too much on the modelâ€™s
performance.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="paper-summary" /><summary type="html"><![CDATA[This is a brief summary of the Distilling the Knowledge in a Neural Network paper.]]></summary></entry><entry><title type="html">a post with table of contents on a sidebar</title><link href="https://zahrabenslimane.github.io/Portfolio/blog/2023/sidebar-table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents on a sidebar" /><published>2023-04-25T14:14:00+00:00</published><updated>2023-04-25T14:14:00+00:00</updated><id>https://zahrabenslimane.github.io/Portfolio/blog/2023/sidebar-table-of-contents</id><content type="html" xml:base="https://zahrabenslimane.github.io/Portfolio/blog/2023/sidebar-table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents as a sidebar.</p>

<h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2>

<p>To add a table of contents to a post as a sidebar, simply add</p>
<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">sidebar</span><span class="pi">:</span> <span class="s">left</span>
</code></pre></div></div>
<p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change <code class="language-plaintext highlighter-rouge">left</code> to <code class="language-plaintext highlighter-rouge">right</code>.</p>

<h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h2 data-toc-text="Customizing" id="customizing-your-table-of-contents">Customizing Your Table of Contents</h2>

<p>If you want to learn more about how to customize the table of contents of your sidebar, you can check the <a href="https://afeld.github.io/bootstrap-toc/">bootstrap-toc</a> documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.</p>

<h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="toc" /><category term="sidebar" /><summary type="html"><![CDATA[an example of a blog post with table of contents on a sidebar]]></summary></entry><entry><title type="html">Audio Source Separation Using Non-Negative Matrix Factorization (NMF)</title><link href="https://zahrabenslimane.github.io/Portfolio/blog/2023/audio-source-separation-using-non-negative-matrix-factorization-nmf/" rel="alternate" type="text/html" title="Audio Source Separation Using Non-Negative Matrix Factorization (NMF)" /><published>2023-02-01T18:40:29+00:00</published><updated>2023-02-01T18:40:29+00:00</updated><id>https://zahrabenslimane.github.io/Portfolio/blog/2023/audio-source-separation-using-non-negative-matrix-factorization-nmf</id><content type="html" xml:base="https://zahrabenslimane.github.io/Portfolio/blog/2023/audio-source-separation-using-non-negative-matrix-factorization-nmf/"><![CDATA[]]></content><author><name></name></author></entry></feed>