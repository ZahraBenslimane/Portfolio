---
layout: post
title:  Brief Summary | Wav2Vec 2.0
date:   2023-04-28 11:40:16
description: This is a brief summary of the Wave2Vec 2.0
tags: paper-summary
related_posts: false
---

Wav2vec 2.0 paper proposes an adapted architecture that leverages a transformer-based language model to better
handle the temporal continuity of speech while minimizing our reliance on label data for training. The training
approach consists of pre-training the model in a self-supervised manner to extract high-level contextualized speech
representation from large amounts of unlabelled, raw, audio data. The models are then fine-tuned to perform downstream
tasks, such as predicting words and phonemes, with a considerably smaller labeled dataset.

The pre-training process involves extracting the latent feature representations 𝑧𝑖 of the raw audio data with a multilayer
CNN. These representations are fed to two different modules. A quantization module that converts them into
discrete vector representations 𝑞𝑖 by sampling from finite codebooks using the Gumbel-Softmax trick. A percentage
of the feature representations 𝑧𝑖 are then masked and fed to a context network, with a transformer architecture similar
to BERT, it learns to predict the correct contextualized representation 𝑐𝑖 or speech units for its masked inputs. The
overall model then solves a contrastive task, by learning to distinguish the true quantized latent representation for
each masked input among other quantized representations that act as distractors. The objective function minimizes
the cosine similarity between the transformer’s projected context vectors 𝑐𝑖 with the quantized representations 𝑞𝑖
(Contrastive loss) while making sure the model encourages diversity when sampling codewords from codebooks
(Diversity loss).

During the supervised fine-tuning, we freeze the quantization module and train new task-specific layers. For example,
in a speech recognition-related task, a linear projection is added to minimize the Connectionist Temporal
Classification (CTC) loss. By that, Wav2Vec 2.0 was able to achieve state-of-the-art performance on a range of
speech recognition datasets.

----

I found the wav2vec 2.0 paper challenging. The authors draw upon a diverse set of research
areas, including contrastive predictive coding, product quantization, and masked predictions with transformers to
create a comprehensive framework for speech recognition. Additionally, the paper references several other works,
making it a dense read that requires a strong understanding of the underlying concepts. Nevertheless after following
the continuous train of thought from the <a href="https://arxiv.org/abs/1807.03748">Contrastive Prediction Coding</a> paper, to <a href="https://arxiv.org/abs/1904.05862">Wav2vec</a>  and <a href="https://arxiv.org/abs/1910.05453">
VQ-wav2vec</a> and getting familiar with <a href="https://arxiv.org/abs/1810.04805"> BERT</a>, the Wav2Vec 2.0 paper approach became more apparent.




