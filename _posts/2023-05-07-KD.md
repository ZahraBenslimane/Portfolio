---
layout: post
title:  Brief Summary | Distilling the Knowledge in a Neural Network
date:   2023-05-07 10:40:16
description: This is a brief summary of the <a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a> paper.
tags: paper-summary
categories: sample-posts
---

Paper : <a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a>

The authors proposed an architecture-agnostic technique called knowledge distillation, which involves training a
small model to mimic the behavior of a cumbersome model, referring to a single large model or an ensemble of
models. To train the small model to generalize in the same way as the cumbersome model, it is as important to
learn the target outputs of the model, as it is to learn the ratios of the small probabilities of the incorrect targets.
However, these small probabilities tend to be suppressed by the softmax function that greatly highlights the value of
the maximum output logit, minimizing their influence on the cross-entropy cost function. The authors figured that
by raising the temperature parameter in the final softmax function, it produces a softer probability distribution over
classes, called soft targets, resulting in a higher entropy, thus, more information to learn from.

The main idea is to first train a large network with a softmax temperature equal to 1, then feed a transfer set of unlabeled or labeled data to this model and compute their soft targets with a temperature T, higher than one. Knowledge is
then transferred to the small model, by training it to align its soft targets, computed with the same high-temperature
T used for generating the transfer set, with the soft targets produced by the cumbersome model. A weighted average
of two objective functions is used for training the distilled model, the cross entropy of the soft targets of the two
models, and the cross entropy of the distilled model with the correct target class.

Additionally, the paper presents a creative way of training big ensembles as our cumbersome model. A generalist
model is trained on all training data classes and multiple specialist models are trained in parallel to focus on clustered
subsets that are more confusable to the generalist model.

The paper highlights the fact that different requirements are set for training and inferring neural network models and
encourages first focusing on achieving the best performance of the model regardless of the computational cost and
then transferring the knowledge to a more resource-contrained network. Knowledge distillation is general enough
to be applied to different fields and has shown great results in compressing model architecture and transferring
the knowledge of an ensemble of models into a single network, without compromising too much on the modelâ€™s
performance.

